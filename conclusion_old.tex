\section{Conclusions and future work}

%%SL.1.22: Here is a suggested rephrasing of the paragraphs below (similar content, just organized a bit differently):
% We presented a model-based reinforcement learning approach that can operate directly on raw pixel observations, and can learn effective policies to play games in the Atari Learning Environment. Our experiments demonstrate that our approach can learn to play many of the games with just $100$K transitions, corresponding to ??? hours of play time. In many cases, the number of samples required for prior methods to learn to reach the same reward value is several times larger.
%
% While the predictive model in our experiments has stochastic latent variables, the Atari games that we evaluate on are largely deterministic. Studying stochastic domains is an exciting direction for future, and we expect that effective probabilistic modeling of dynamics would be effective in such settings also. Another interesting direction for future work is to study other ways in which the predictive neural network model could be used. Our approach utilizes the model as a learned simulator, and then directly uses model-free policy search methods to acquire the behavior policy. However, since neural network models are differentiable, the additional information contained in the dynamics gradients could itself be incorporated into the reinforcement learning process. Finally, the representation learned by the predictive model is likely to itself be more meaningful than the raw pixel observations from the environment, and incorporating this representation into the policy could further accelerate and improve the reinforcement learning process.
%
% As a long-term challenge, we believe that model-based reinforcement learning based on stochastic predictive models represents a promising and highly efficient alternative to model-free RL, and applications of such approaches to both high-fidelity simulated environments and real-world data represent an exciting direction for future work that can enable highly efficient learning of behaviors from raw sensory inputs, in domains such as robotics and autonomous driving.

%%SL.1.22: Some general rationale: let's try to keep prior work to the prior work section, it often comes across as pretty unsatisfying to have a discussion section that says something like "prior word did this, maybe we can do it too in future work" (if it was done in prior work, then it's not future work). It's also important to leave the reader with a very positive impression of the work, so while discussing some limitations is really important, it's also a good idea to use these to frame exciting future work that goes beyond the current method.

We presented a number of ideas that, taken together, allow to train Atari games end-to-end in a model-based way and get improvements in data efficiency.
While in this work we focus on deterministic environments, it would be interesting to study stochastic environments, possibly exploiting methods from \cite{world_models}. This would open possibly of using model-based reinforcement learning techniques in real-world unstructured scenarios, for example competing in a video game against human players. 

In some of our experiments we output probability distributions over pixel values. We conjecture that their entropy/variance can serve a measure how model is confident about its own predictions. We mention \cite{trpo_ensemble} which introduced model ensembles with a similar purpose. Measuring confidence is a challenging task with multiple possible applications. On the one hand, it might be used to determine trust regions for policy optimization, resulting in a more stable execution of Algorithm~\ref{alg:basic_loop}. On the other hand, creating an incentive to enter low confidence regions might be a useful exploration strategy. 

Finally, there are many benefits of having a neural net model of a game. Its hidden state is smaller and possibly more semantically meaningful than visual input and could be used as an input for policy. As called for in \cite{modelplanning}, one could use it for model-based planning and propagate gradients though the model. Finally, there are many technical benefits as the neural network model runs on the same accelerator that trains the policy and can be ran in batch mode, greatly simplifying the architecture of the RL setup and speeding up computations for more complex environments.

As a long-term challenge, we would like to deal with visually rich simulated robotics environments such like Carla \cite{carla}. Compressing states as shown in \cite{world_models} is particularly appealing as such approach could be used in mixed scenario where the compression is learn using simulated data and later used for rapid reinforcement learning on real-world data. 