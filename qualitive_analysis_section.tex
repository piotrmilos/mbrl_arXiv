\title{Qualitative analysis to be}
\author{}
\date{\today}
\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}

\usepackage{hyperref}
\begin{document}
\maketitle

\section{Qualitative analysis}
This section provides a qualitative analysis and case studies of individual games. We emphasize that we did not adjust the method individually for each game, but provide this analysis of individual games for the benefit of the reader. We strongly recommend the reader to watch videos.\footnote{The videos consists of three panes, the left is imagined rollout generated by a model, the middle is the ground-truth game rollout, the right one presents the difference. On top we report the obtained reward and the cumulative reward since reset.}

\subsection{Pixel perfect sequences}
In some cases our models were able to predict future perfectly, up to a single pixel! As far as we know, our models are first to achieve that.\footnote{check that!} This property holds for rather short time intervals (up to XYZ frames), extending it to long sequences is a very exciting research direction. We present visualizations of this for breakout, freeway and pong.

[see breakout, freeway, pong in \href{https://drive.google.com/open?id=1tXbwvKjO1-rVWQ88UJE1plOpzypGVXr0}{MBLR\_videos/pixel\_perfect}]

\subsection{Benign mistakes}
Even though of the above positive examples, perfect models seems elusive, especially at early stages of learning. It is believed that model-based RL must be able to handle model errors. Interestingly, in some cases we observed our models to divert from the original game in a way which was harmless or only mildly harmful for the policy training. 

[see bowling, crazy\_climber, kung\_fu\_master, pong in \href{https://drive.google.com/open?id=1vQlJMlThZEvN6AmqSvhSA9NnMQFA3e5T}{MBLR\_videos/benign\_errors}]

For bowling and pong we observe that the ball splits into two. While unphysical, on the accompanied videos one can observe that these errors do not distort much the objective of the game. Similarly, in kung fu master or model diverts from the ground truth by spawning a different number of opponents! In crazy climber we see a bird changing its direction.

The two last cases can be possibly attributed to the fact that we use a stochastic model. Though not aligned with the real environment the predicted behaviors seem perfectly plausible.\footnote{Evaluating stochastic models is generally hard, anyone comment on that?} \footnote{Should we proceed with the discussion stochastic vs deterministic. Explain why stochastic models might be better?} 

\subsection{Hard errors}
Unfortunately in some cases our models failed badly. 

[see asteroids, atlantis, battle\_zone, private\_eye in \href{https://drive.google.com/open?id=1Y9jlYk-UkrmJY1rcEU9MnXr1iaW25lGI}{hard\_errors}]

For atlantis and battle\_zone the failure is due to very small size of the bullets. This calls for better training methods, which take into account how relevant various entities are for the RL problem.\footnote{This might sound too general/fuzzy} In asteroids our model failed to learn that the agent ships teleports to a pseudo-random location upon hitting fire.\footnote{better movie needed} Finally, private\_eye is an example of the game for which the agent traverses various scenery, teleporting from one to the other. Our model fails to capture these transitions. 

Another category of severe errors is when the model fails to predict rewards.

[see amidar, assultin \href{https://drive.google.com/open?id=1yn9xk1Ir365g7bD2YtnbY8eoEEl9CDfF}{bad_reward}]

\subsection{Model based loop success vs failure}
Freeway is a game with particularly easy mechanics, though somewhat long exploration. We present a snapshot of random exploration.

[include movie]

This environment makes appearance in \cite{recurrent} and is considered as an easy environment to model (see page 18 there). The story is more complicated in the model-based reinforcement learning setting. In one of our experiments, we observed that some of the runs scored the maximal result ($30$+), while others completely failed. We conjecture the following scenario. If at early stages the entropy of the policy decayed too rapidly the collected experience stayed limited leading to poor world model, which was not powerful enough to support exploration (e.g., the chicken disappears when moving to high). In one of our experiment we observed that the final policy was that the chicken moved up only to the second lane and stayed waiting to be hit by car and so on so forth. 

\section{Tentative sections}
\subsection{Comparison of various models}
For example in assult the stochastic discrete model imagines different but plausible scenarios, sv2p blurs everything; deterministic fails to create new ships.

\subsection{Other analyses}
For one example make an in-depth analysis e.g. a) why breakout has only mediocre results, b) what is the change of score of pong over the epochs and why, c) is fishing rod in fishing derby to thin ;), d) why is it hard to catch the gopher, e) pick a game with good and bad results, try to do comparative analysis, f) train a model on a lot of data (say 10mln samples from various polices) and check if this can be used to get better, g) any good case study test for overfitting in RL training.
\newpage 
\bibliographystyle{abbrv}
\bibliography{main}

\end{document}