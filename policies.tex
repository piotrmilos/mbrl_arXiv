%!TEX root = /Users/piotrmilos/PycharmProjects/mbrl_arXiv/main.tex
\section{Policy Training} \label{sec:policy_training}
We will now describe the details of SimPLe, outlined in Algorithm \ref{alg:basic_loop}.  In step 6 we use the proximal policy optimization (PPO) algorithm~\cite{ppo} with $\gamma=0.95$. The algorithm generates rollouts in the simulated environment $env'$ and uses them to improve policy $\pi$. The fundamental difficulty lays in imperfections of the model compounding over time. To mitigate this problem we use short rollouts of $env'$. Typically every $N=50$ steps we uniformly sample the starting state from the ground-truth buffer $D$ and restart $env'$. See Section \ref{sec:ablations}, paragraph {\em Steps} and paragraph {\em Random starts} for experimental evaluations. Using short rollouts may have a degrading effect as the PPO algorithm does not have a way to infer effects longer than the rollout length. To ease this problem, in the last step of a rollout we add to the reward the evaluation of the value function. That the value function is learned along with the policy as it serves to calculate the advantage function.

We stress that imperfections in the model are unavoidable, as it is trained using data generated collected in each loop with the current policy. Moreover, the training of the world model is prone to overfitting as we try to gather as little data as possible from the real environment. Finally, sometimes the model fails in a catastrophic way by losing semantic coherence (e.g., a disappearing ball in Pong).

The main loop in Algorithm \ref{dpll} is iterated $15$ times. In our best expriments we trained the world model for $225$K steps in the first iteration and for $75$K steps in each of the following ones; we used batches of size $2$. As they turned out to be very demanding computentionally (taking more than three weeks), when running ablation experiments we used $5$ times less training samples.\footnote{Specifically, in the tables in Appendix \ref{numerical_results}, \emph{SD long} refers to ``long'' $225$K/$75$K training while all others used $45$K/$15$K.} 


In each of the iterations, the agent is trained inside the latest world model using PPO. In every PPO epoch we used $16$ parallel agents, the number of PPO epochs is $z\cdot 1000$, where $z$ equals to 1 in all passes except last one (where $z = 3$) and two passes number 8 and 12 (where $z = 2$). This gives $800$K$\cdot z$ interactions with the simulated environment in each of the loop passes. In the process of training the agent performs  $15.2$M interactions with the simulated environment $env'$.
