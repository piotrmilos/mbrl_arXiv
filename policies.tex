\section{Policy Training} \label{sec:policy_training}
%TODO(pm): put correct reference to ablations
We will now describe the details of SimPLe, outlined in Algorithm \ref{alg:basic_loop}.  In step 6 we use the proximal policy optimization (PPO) algorithm~\cite{ppo} with $\gamma=0.95$. The algorithm generates rollouts in the simulated environment $env'$ and uses them to improve policy $\pi$. The fundamental difficulty lays in imperfections of the model compounding over time. To mitigate this problem we use short rollouts of $env'$. Typically every $N=50$ steps we uniformly sample the starting state from the ground-truth buffer $D$ and restart $env'$. See Section \ref{sec:ablations}, paragraph {\bf Steps} and paragraph {\bf Random starts} for experimental evaluations of different values of $\gamma, N$. Using short rollouts may have a degrading effect as the PPO algorithm does not have a way to infer effects longer than the rollout length. To ease this problem, in the last step of a rollout we add to the reward the evaluation of the value function. That the value function is learned along with the policy as it serves to calculate the advantage function.

We stress that imperfections in the model are unavoidable, as it is trained using data generated collected in each loop with the current policy. Moreover, the training of the world model is prone to overfitting as we try to gather as little data as possible from the real environment. Finally, sometimes the model fails in a catastrophic way by losing semantic coherence (e.g., a disappearing ball in Pong).

%TODO(pm): This is false in our best long expseriments!!
The main loop in Algorithm \ref{dpll} is iterated $15$ times. The world model is trained for $45$K steps in the first iteration and for $15$K steps in each of the following ones; we used batches of size $2$. Shorter training in later iterations does not degrade the performance because the world model after the first iteration captures already part of the game dynamics and only needs to be extended to novel situations.

In each of the iterations, the agent is trained inside the latest world model using PPO. In every PPO epoch we used 16 parallel agents collecting 25, 50 or 100 steps from the simulated environment $env'$, see Section \ref{sec:ablations}, paragraph {\bf Steps} for ablations with regard to number of steps. The number of PPO epochs is $z\cdot 1000$, where $z$ equals to 1 in all passes except last one (where $z = 3$) and two passes number 8 and 12 (where $z = 2$). This gives $800$K$\cdot z$ interactions with the simulated environment in each of the loop passes. In the process of training the agent performs  $15.2$M interactions with the simulated environment $env'$.
%TODO(pm): put reference to Figure \ref{fig:Cdf} if still valid