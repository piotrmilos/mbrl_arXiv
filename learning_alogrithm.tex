\section{Simulated Policy Learning (SimPLe)}
\label{subsec:model}
\label{sec:mbrl}


Reinforcement learning is formalized in Markov decision processes (MDP). An MDP is defined as a tuple $(\mathcal{S}, \Aa, P, r, \gamma)$, where $\mathcal{S}$ is a state space, $\Aa$ is a set of actions available to an agent, $P$ is the unknown transition kernel, $r$ is the reward function and $\gamma\in (0,1)$ is the discount factor. 
In this work we refer to MDPs as environments and assume % that environments fulfill the following assumptions:
that environments do not provide direct access to the state (i.e., the RAM of Atari 2600 emulator). Instead we use visual observations, typically $210\times 160$ RGB images. A single image does not determine the state. To circumvent this, we stack the four previous frames, using the result as the state.
A reinforcement learning agent interacts with the MDP by issuing actions according to a policy. Formally, policy $\pi$ is a mapping from states to probability distributions over $\mathcal{A}$. The quality of a policy is measured by the value function $\mathbb{E}_{\pi}\left(\sum_{t=0}^{+\infty}\gamma^t r_{t+1}|s_0=s \right)$, which for a starting state $s$ estimates the total discounted reward gathered by the agent. In Atari 2600 games our goal is to find a policy which maximizes the value function from the beginning of the game.
Crucially, apart from an Atari 2600 emulator environment $env$ we will use \textit{a neural network simulated environment} $env'$ which we call a \textit{world model} and describe in detail in Section \ref{sec:world_models}. The environment $env'$ shares the action space and reward space with $env$ and produces visual observations in the same format, as it will be trained to mimic $env$.  Our principal aim is to
train a policy $\pi$ using a simulated environment $env'$ so that $\pi$ achieves good performance in the original environment $env$. In this training process we aim to use as few interactions with $env$ as possible. 
The initial data to train $env'$ comes from random rollouts of $env$. As this is unlikely to capture all aspects of $env$, we use the data-aggregation iterative method presented in Algorithm~\ref{alg:basic_loop}.

\begin{figure}  % \small
\removelatexerror
\begin{algorithm}[H]
\caption{Pseudocode for SimPLe}\label{dpll}
\begin{algorithmic}
\STATE Initialize policy $\boldsymbol\pi$
\STATE Initialize model parameters $env'$
\STATE Initialize empty set $\mathbf{D}$
\WHILE{not done} 
\STATE \underline{$\triangleright$ collect observations from real env.}
\WHILE{not enough observations}
\STATE $a \gets \boldsymbol\pi(s)$
\STATE $(s' ,r) \gets env(a)$
\STATE $\mathbf{D} \gets \mathbf{D} \cup (s,a,r,s')$
\STATE $s \leftarrow s'$
\ENDWHILE
\STATE \underline{$\triangleright$ update model using collected data.}
\STATE $\boldsymbol\theta \gets \text{TRAIN\_SUPERVISED}(env', \mathbf{D})$
\STATE \underline{$\triangleright$ update policy using world model.}
\STATE $\boldsymbol\pi \gets \text{TRAIN\_RL}(\boldsymbol\pi, \boldsymbol\theta)$
\ENDWHILE
\end{algorithmic}
\label{basic_loop}
\label{alg:basic_loop}
\end{algorithm}
\end{figure}